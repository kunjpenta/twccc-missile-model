TEWA Backend Structure (as of now)

1. Project Root
   missile_model/
   ├─ core/
   ├─ tewa/
   ├─ templates/
   ├─ pg_excel_exports/
   ├─ manage.py
   ├─ readme.MD
   ├─ try.py

core/ – Common utilities, base models, tests, and views not specific to TEWA.

tewa/ – Main TEWA application containing all models, services, APIs, tasks, and management commands.

templates/ – Any Django template files for admin/UI.

pg_excel_exports/ – (Optional) CSV/Excel export utilities.

manage.py – Django management script.

try.py – Miscellaneous script (likely for testing).

2. core/
   core/
   ├─ admin.py
   ├─ apps.py
   ├─ models.py
   ├─ views.py
   ├─ urls.py
   ├─ utils/
   ├─ tests/
   │ ├─ test_kinematics.py
   │ ├─ test_scoring.py
   │ ├─ test_tewa_kinematics.py
   │ ├─ test_units_geodesy.py
   │ └─ quick_check.py

Provides common models, utilities, and test modules used across TEWA.

Tests include:

Kinematics computations (CPA/TCPA/TDB/TWRP)

Threat scoring

Units & geodesy conversions

utils/ – Geodesy, unit conversion, helper functions.

3. missile_model/
   missile_model/
   ├─ settings.py
   ├─ urls.py
   ├─ wsgi.py
   ├─ asgi.py
   ├─ celery.py

Core Django project configuration.

celery.py – Celery app for periodic tasks.

urls.py – Includes global URL patterns and TEWA API routes.

4. tewa/
   tewa/
   ├─ admin.py
   ├─ apps.py
   ├─ models.py
   ├─ serializers.py
   ├─ forms.py
   ├─ views.py
   ├─ urls.py
   ├─ types.py
   ├─ tasks.py
   ├─ services/
   │ ├─ engine.py
   │ ├─ threat_compute.py
   │ ├─ ranking.py
   │ ├─ kinematics.py
   │ ├─ scoring.py
   │ ├─ normalize.py
   │ ├─ sampling.py
   │ └─ csv_import.py
   ├─ management/
   │ └─ commands/
   │ ├─ compute_threats.py
   │ ├─ import_tracks.py
   │ └─ seed_demo.py
   ├─ fixtures/
   │ └─ tewa_seed.json
   ├─ migrations/
   └─ tests/
   ├─ test_api_compute_at.py
   ├─ test_compute_multiple_scenarios.py
   └─ test_csv_import.py

Purpose of Key Directories & Files

models.py – Core TEWA models (DefendedAsset, Track, TrackSample, ThreatScore, Scenario, ModelParams)

serializers.py – DRF serializers for TEWA models.

forms.py – Admin/UI forms, e.g., for DA creation and validation.

urls.py – API routes for TEWA endpoints.

views.py – API views (compute, ranking, DA/Track CRUD).

types.py – Type hinting for ParamsLike and other structures.

tasks.py – Celery tasks for periodic computation.

services/ – Core business logic:

engine.py – Scenario engine to compute scores.

threat_compute.py – Compute and persist threat scores.

ranking.py – Threat ranking logic.

kinematics.py – CPA/TCPA/TDB/TWRP calculations.

scoring.py – Normalization and scoring functions.

normalize.py – Helper math functions (clamp01, inv1).

sampling.py – Track interpolation at specific timestamps.

csv_import.py – CSV import for tracks and TrackSamples.

management/commands/ – Django management commands:

seed_demo.py – Seed sample scenarios, DAs, tracks.

compute_threats.py – Compute all threat scores (manual or periodic).

import_tracks.py – Import tracks from CSV.

fixtures/tewa_seed.json – Initial dataset for demo scenario.

tests/ – Unit tests for API, CSV import, compute engine.

5. APIs & Frontend Hints

/api/tewa/compute_at/ – Compute threat scores at a timestamp.

/api/tewa/compute_now/ – Trigger immediate compute via Celery.

/api/tewa/ranking/ – Get ranked threats (top-N, per-DA optional).

/api/tewa/da/ – CRUD for defended assets.

/api/tewa/track/ – CRUD for tracks.

/api/tewa/tracksample/ – Track sample snapshots.

/api/tewa/score/ – Get persisted ThreatScores.

Data flows:

Track + DA + Scenario → Compute Scores → Persist ThreatScore → Rank Threats → Frontend fetches via /ranking/.

TEWA Backend — Full Technical Overview

1. Project Purpose

The TEWA (Threat Evaluation & Weapon Assignment) backend is a deterministic, high-fidelity simulation engine that computes threat scores for aircraft tracks relative to defended assets (DAs) in various scenarios. Its primary function is:

Track aircraft movement over time.

Compute kinematics-based threat scores (CPA/TCPA/TDB/TWRP) relative to DAs.

Persist results for ranking and historical analysis.

Support automated and periodic computation for training, decision-making, or wargaming applications.

The backend is designed to be agnostic of the frontend, which is Angular-based. The backend exposes RESTful APIs for data access, CSV uploads, computation requests, and threat rankings.

2. Tech Stack
   Layer Technology / Library Role
   Backend Python 3.10, Django 5.1.x Web framework, ORM, management commands
   Database PostgreSQL (+ PostGIS optional) Stores scenarios, DAs, tracks, track samples, model parameters, and threat scores
   Asynchronous Tasks Celery (optional) Scheduled periodic computation (e.g., hourly updates)
   Serialization / API Django REST Framework style JSON-based REST endpoints for frontend integration
   Time & Dates UTC everywhere, ISO8601 Ensures consistent computation and timestamping
3. Project Structure
   missile_model/
   ├─ manage.py
   ├─ missile_model/
   │ ├─ settings.py # Django settings, DB config, timezone = UTC
   │ ├─ urls.py # Root URLs, includes /api/tewa/
   │ ├─ celery.py # Celery app definition
   ├─ core/ # Utilities and test modules
   │ ├─ utils/
   │ │ ├─ geodesy.py # Distance, bearing, ENU conversions
   │ │ └─ units.py # Unit conversions: m/s, km, etc.
   │ └─ tests/ # Unit tests for kinematics, scoring, conversions
   └─ tewa/
   ├─ models.py # Core models: Scenario, Track, TrackSample, DA, ModelParams, ThreatScore
   ├─ serializers.py # DRF serializers for API endpoints
   ├─ api/
   │ ├─ views.py # API endpoints: compute_now, compute_at, score, ranking, upload_tracks
   │ └─ urls.py
   ├─ services/
   │ ├─ kinematics.py # CPA, TCPA, TDB, TWRP computations
   │ ├─ normalize.py # Normalization functions (inv1, clamp01)
   │ ├─ scoring.py # Weighting & final threat score calculation
   │ ├─ threat_compute.py # Computes & persists ThreatScore for tracks × DAs
   │ ├─ engine.py # Time-aware compute engine
   │ ├─ sampling.py # Interpolates TrackSamples (linear/latest)
   │ └─ csv_import.py # Robust CSV importer (tz-aware)
   ├─ management/commands/
   │ ├─ seed_demo.py # Seeds scenarios, tracks, DAs, model params
   │ ├─ import_tracks.py
   │ └─ compute_threats.py # Periodic threat score computation
   └─ fixtures/tewa_seed.json # Default seed data

4. Data Model Overview
   Model Purpose Key Fields
   Scenario Represents a TEWA simulation scenario name, start_time, end_time, notes
   DefendedAsset (DA) Represents a fixed defended location name, lat, lon, radius_km
   Track Represents an aircraft / moving object track_id, lat, lon, alt_m, speed_mps, heading_deg, scenario
   TrackSample Snapshot of Track at a specific time track, t (timestamp), lat, lon, alt_m, speed_mps, heading_deg
   ModelParams Scenario-specific scoring weights & normalization w_cpa, w_tcpa, w_tdb, w_twrp, cpa_scale_km, etc.
   ThreatScore Persisted threat score per (Track × DA × Scenario) track, da, scenario, score, computed_at, raw_components

Key Features:

Indexes exist on track_id, scenario, da to optimize queries.

Timestamps are always UTC-aware; frontend passes ISO8601 strings with Z.

5. CSV Upload Pipeline

Purpose: Bulk import tracks and samples.

API Endpoint: POST /api/tewa/upload_tracks/

CSV Format Required:

track_id,lat,lon,alt_m,speed_mps,heading_deg,timestamp

Backend Workflow:

CSV is read line by line (csv.DictReader).

Timestamps are parsed via \_parse_ts_aware_utc() → UTC-aware datetimes.

Track: created if not exists (unique per scenario + track_id).

TrackSample: created for each row.

Latest track snapshot updated for real-time compute.

Returns summary JSON:

{
"message":"Upload processed",
"tracks_created":3,
"samples_created":3,
"errors":[]
}

6. Kinematics & Threat Scoring
   CPA/TCPA/TDB/TWRP

CPA: Closest point of approach (distance to DA).

TCPA: Time to CPA (seconds).

TDB: Distance to DA boundary circle.

TWRP: Time to weapon release (based on weapon range).

Normalization

normalize.inv1(x, scale) → 1 / (1 + x/scale)

Values are clamped [0,1].

Negative TCPA (past events) is down-weighted.

Scoring

Weighted sum of normalized CPA, TCPA, TDB, TWRP using ModelParams.

Result persisted in ThreatScore.

7. Compute Engine (engine.py)

Input: Scenario, timestamp (when), method (linear/latest), optional DAs.

Steps:

Interpolate TrackSamples to requested time.

Compute kinematics for each Track × DA pair.

Normalize and weight using ModelParams.

Persist ThreatScore.

Output: Threats ready for ranking or historical view.

API Exposure:

POST /compute_now/ → current snapshot computation.

POST /compute_at/ → timestamped computation.

GET /score/ → list persisted scores (filterable).

GET /ranking/ → top-N threats per scenario.

8. Periodic Threat Computation

Implemented as management command: compute_threats.py.

Loops through all active scenarios.

Computes ThreatScore for each scenario using the two constant DAs (DA-Alpha, DA-Bravo).

Optional integration with Celery Beat for hourly/daily scheduled computation.

9. Data Flow
   [Frontend] ---> [Django API] ---> [Database]
   Upload CSV POST /upload_tracks Insert Tracks/TrackSamples
   Compute Now POST /compute_now Calculate ThreatScore & persist
   Compute At POST /compute_at Interpolate + Compute + Persist
   Fetch Ranking GET /ranking Fetch top-N ThreatScore per scenario

Internal Flow (Compute):

TrackSamples → interpolated to requested timestamp

Kinematics → CPA/TCPA/TDB/TWRP

Normalize & Weight → ThreatScore

Persist → DB for frontend query

10. Fixtures & Seed Data

tewa/fixtures/tewa_seed.json contains:

3 Scenarios (Demo-Scenario, Scenario-2, Scenario-3)

2 constant DAs (DA-Alpha, DA-Bravo)

Tracks & TrackSamples for each scenario

ModelParams for scoring

Benefit: Running loaddata instantly seeds backend with functional data.

11. Best Practices Implemented

Timezone-safe: all timestamps UTC-aware, frontend sends ISO8601-Z.

Deterministic: fixed kinematics, fixed DAs → reproducible threat scores.

Idempotent seed: loading fixture multiple times doesn’t duplicate tracks/DA.

Indexes: optimized queries for scenario × DA × track.

Unit-tested: CSV import, kinematics, scoring, geodesy conversions.

12. Outstanding / Next Features

Pagination and filtering on /score/.

Optional scenario-scoped DA lists (/scenarios/{id}/das/).

Celery Beat integration for fully automated periodic computation.

Optional “calculate_scores” endpoint for pure compute without persistence.

Enhanced CSV error reporting (row-level messages).

✅ Summary

The backend is fully operational:

Tracks, track samples, scenarios, DAs are seeded and persisted.

Threat computation is deterministic, reproducible, and logged.

APIs support upload, compute, and ranking.

Ready for frontend integration or periodic automated execution.

##############################################################################13/10/2025####################################################################

got it — here’s the “why it works” version, focused only on principles and theory (no code).

Core ideas behind the backend you built

1. What we’re modeling

You’re estimating a “threat” level from a moving track to a defended asset (DA). The threat is not a single raw measurement; it’s a composite of several cues that each capture a different intuition:

CPA (closest point of approach, in km): distance at the moment the track comes closest to the DA. Smaller is more threatening.

TCPA (time to CPA, in seconds): how soon that closest approach will happen. Sooner is more threatening.

TDB (distance to DA center, in km): how far the track is from the DA right now. Closer is more threatening.

TWRP (time to weapon release, in seconds): how long until a weapon could be released against the DA. Sooner is more threatening; if release isn’t possible, it should reduce threat.

The theory is that no single metric is enough. A track may be close but already passed by (negative TCPA), or far but closing very quickly; a weighted combination captures this nuance.

2. Normalization: making unlike things comparable

CPA is in kilometers; TCPA and TWRP are in seconds; TDB is also km. You need a way to place them on the same scale so their weighted sum is meaningful.

The principle: convert each raw component to a 0–1 “urgency” score where 1 ≈ very threatening and 0 ≈ not threatening. We do this by inverting a simple ratio:

“Smaller is worse” becomes “1 minus (value ÷ scale).”

The scale is a tunable knob that says “what does ‘typical’ look like?” A CPA scale of 20 km says “inside 20 km should feel urgent.”

Clipping (a.k.a. clamping) the result to 0–1 aligns with human intuition and keeps outliers from exploding your sums.

Why inversion instead of more exotic functions? It’s monotonic, easy to reason about, and transparent when you tune scales. You can always upgrade to nonlinear shapes later if ops demand it.

3. Weights: expressing operational priorities

Not all components matter equally. You capture priorities by weights that sum near 1 (not required, but intuitive). Examples:

If positional proximity matters most in your doctrine, the CPA weight is high.

If timelines are the driver (e.g., short window for intercept), TCPA weight gets emphasized.

Changing weights should change rankings. That’s a core property you validated: a “near but later” track can outrank a “far but soon” one if your doctrine says proximity dominates, and vice versa.

4. Handling tricky edge cases (so you don’t lie to yourself)

Real-world data is messy. Your rules here prevent accidental optimism or pessimism.

Negative TCPA means CPA occurred in the past. Conceptually, urgency from “time to event” is zero, not negative. You treat negative TCPA as contributing nothing to urgency.

TWRP can be “no opportunity” (e.g., geometry or range). Representing that as infinity and mapping it to zero urgency communicates “no threat from this axis right now,” without breaking math.

Bad inputs (None, NaN, infinities) degrade gracefully to zero contribution, instead of poisoning the score with nonsense.

This preserves the meaning of the final score: greater numbers are actually more threatening, not just numerically bigger.

5. Clamping the final score (and when not to)

You allow an optional final clamp of the weighted sum to 0–1. Why optional?

Clamped scores are great for dashboards and human decision-making.

Unclamped scores are great for testing and calibration; if you double all weights, the total should double. That property gets hidden by clamping.

This is a pragmatic toggle: clamp for operations, disable for diagnostics.

6. Separation of concerns: compute vs persist

You separated “pure computation” from “persistence to the database.”

Pure functions: take inputs (track state, DA position, parameters), return a score and components. No side effects. They are predictable, testable, and reusable (e.g., simulation, analytics).

Persistence functions: take those results and store them as rows (e.g., ThreatScore). They orchestrate retrieval of tracks, parameters, and write new records.

Why this matters: it’s the difference between a math engine and an application. You can test the math engine thoroughly without spinning up the whole app stack.

7. Parameter coercion: one function, many callers

Parameters can come from:

A dict (unit tests, batch scripts).

A database model (runtime).

A “coercion” step standardizes both into the same internal shape with defaults. This avoids duplicating logic, keeps defaults consistent, and makes every code path behave the same for the same inputs.

8. Sampling/Interpolation: making time explicit

When scoring at a point in time (e.g., from an API endpoint), you can’t assume tracks only exist at recorded timestamps. Interpolation gives a physically plausible state “between” samples.

Linear interpolation is chosen for simplicity and transparency.

It lets you ask “what’s the threat at t?” instead of “what’s the nearest logged point?” which reduces aliasing and timing artifacts.

You’re aligning the computation with the operational question asked.

9. Test philosophy: proving properties, not just lines

Your tests focus on semantic properties:

Normalization behaves intuitively (half-scale ≈ 0.5 urgency).

Negative TCPA reduces overall threat vs. a similar positive TCPA.

Weight emphasis changes rank orderings.

Scores are bounded when clamped, and exceed 1 when intentionally unclamped.

“No release opportunity” is reflected as lower threat.

This is stronger than checking exact numbers: it asserts the model’s behavior and guards against regressions when you tweak scales or weights later.

10. Coverage: confidence, not vanity

Coverage is a tool to detect blind spots, not a score to chase blindly.

High coverage in the math and combination logic matters most.

Lower coverage in views/admin is fine early, unless those paths carry risk.

An HTML coverage report helps you focus on untested branches (e.g., zero-speed kinematics, uncommon geometry) that could fail at the worst time.

Use coverage to steer test investment, not to pretend reality is perfect.

How these choices help you in production

Interpretability: every part of the score has a plain-language meaning. Ops can tune scales and weights and understand the effect.

Robustness: messy inputs don’t crash the system or inflate threat; they degrade to conservative, bounded behavior.

Extensibility: you can add new components (e.g., intent inference, sensor credibility) by normalizing and weighting them into the same framework.

Auditability: separating compute from persistence, and returning components along with the score, makes it easy to explain “why this track was ranked high” in post-mortems.

What to tune next (principles-first)

Calibrate scales against historical engagements or SMEs: “At what CPA and TCPA did we consider an intercept mandatory?” Set scales so 0.7–0.9 aligns with those thresholds.

Consider nonlinear maps: if urgency ramps very fast near the DA, replace linear inversion with a curve (e.g., logistic) for CPA or TDB while keeping the same 0–1 semantics.

Add uncertainty handling: discount components by confidence (e.g., sensor quality, track stability), so a flaky track scores lower given the same geometry.

Introduce scenario doctrine packs: pre-defined weight/scale profiles for different missions (point defense vs. area defense).

That’s the conceptual backbone of what you’ve built: a principled, interpretable, and extensible threat-scoring pipeline grounded in simple, rigorous normalization and clear operational semantics.

Backend Dev Handoff — Task 21 (Score Breakdown View)
Goal

Expose per-threat score breakdown (CPA, TCPA, TDB, TWRP) with raw values, normalized values, weights, per-component contributions, final score, timestamp; provide JSON API + minimal HTML partial/modal.

Current Status

Endpoint: GET /api/tewa/score-breakdown (no trailing slash)

Query params (required): scenario_id (int), track_id (int PK or public string), da_id (int)
Optional: at (ISO-8601 UTC) to select a snapshot <= at; otherwise use latest.

Response: Includes metrics, normalized, weights, contributions, score, total_score (legacy alias), computed_at, and legacy flat fields (cpa_km, tcpa_s, tdb_km, twrp_s) for backward compatibility.

Tests: All existing tests pass after restoring model serializers and ensuring total_score. If one test fails, view layer now backfills total_score from score/final_score.

Files Touched

1. Routing

missile_model/urls.py

path("api/tewa/", include("tewa.api.urls"))

tewa/api/urls.py

path("score-breakdown", views.score_breakdown, name="score-breakdown")

(Keeps existing routes unchanged; no trailing slash by design.)

2. View (new endpoint)

tewa/api/views.py

Adds score_breakdown(request) handler

Validates params; returns:

422 missing/invalid

400 bad at

404 not found

500 generic error

Uses ScoreBreakdownSerializer to shape response

View-level guard: if total_score is None, fills from score or final_score.

3. Serializer (Task 21 + legacy passthroughs)

tewa/api/serializers.py

Restored model serializers expected by views_read.py:

ScenarioSerializer, DefendedAssetSerializer, TrackSerializer, TrackSampleSerializer, ThreatScoreSerializer

Added Task 21 structured serializers:

MetricsSerializer, NormalizedSerializer, WeightsSerializer, ContributionsSerializer, ParamsSerializer

ScoreBreakdownSerializer fields:

scenario_id, track_id, da_id, computed_at, metrics, normalized, weights, contributions, score, params, explain

Legacy passthroughs: cpa_km, tcpa_s, tdb_km, twrp_s, total_score (all optional/nullable)

4. Service (non-breaking single entry point)

tewa/services/score_breakdown_service.py

Public API:

get_score_breakdown(scenario_id: int, track_id: str, da_id: int, at_iso: Optional[str]) -> Dict[str, Any]

Behavior:

Validates Scenario and DefendedAsset exist (404 semantics).

Compute path (preferred): calls existing tewa.services.score_breakdown.get_score_breakdown with persist=True.

Normalizes computed_at to UTC ISO.

Ensures legacy flat fields via \_ensure_legacy_flat_fields.

Backfills score/total_score from best available source: score → final_score → latest ThreatScore.score.

Fallback path: if compute raises MultipleObjectsReturned or any exception, or data insufficient:

Resolves track_id either as PK or latest row with given track_id string.

Fetches latest ThreatScore row (optionally <= at).

Shapes response:

metrics.cpa_m (km→m), metrics.tdb_s proxy from tdb_km assuming nominal closure 250 m/s (keeps old logic untouched).

Legacy flat fields cpa_km, tcpa_s, tdb_km, twrp_s exposed directly from DB row.

score and total_score set from DB row.

normalized/weights/contributions = zeros (can’t reconstruct without compute).

params = {}; explain = static notes.

Helpers:

\_resolve_track_pk (accepts PK or public track_id, picks latest row)

\_latest_threatscore (orders by -computed_at, -id)

\_ensure_legacy_flat_fields (injects top-level cpa_km, etc. from components or derives from metrics.cpa_m)

5. Views import wrapper

tewa/api/views.py continues to import compute/read views to keep old modules stable.

API Contract (as returned)

Example:

{
"scenario_id": 1,
"track_id": "TGT001",
"da_id": 3,
"computed_at": "2025-10-14T10:02:00Z",
"metrics": { "cpa_m": 1234.5, "tcpa_s": 67.8, "tdb_s": 90.2, "twrp_s": 12.0 },
"normalized": { "cpa": 0.82, "tcpa": 0.61, "tdb": 0.34, "twrp": 0.74 },
"weights": { "cpa": 0.35, "tcpa": 0.25, "tdb": 0.20, "twrp": 0.20 },
"contributions": { "cpa": 0.287, "tcpa": 0.153, "tdb": 0.068, "twrp": 0.148 },
"score": 0.656,
"total_score": 0.656,
"params": {},
"explain": [
"Lower CPA → higher normalized threat (inverted scale).",
"Shorter TCPA → higher immediacy risk.",
"TDB gauges time to DA boundary penetration.",
"TWRP indicates time to weapon release window."
],
"cpa_km": 1.2345,
"tcpa_s": 67.8,
"tdb_km": 0.0902,
"twrp_s": 12.0
}

Errors

422 — missing/invalid params (scenario_id, da_id must be ints)

400 — bad at format (non-ISO)

404 — no breakdown for given IDs/time

500 — internal error

How to Use / Sanity Checks

Compute a fresh snapshot (optional):

curl -s -X POST -H 'Content-Type: application/json' \
 -d '{"scenario_id": 1}' \
 "http://127.0.0.1:8000/api/tewa/compute_now/"

List DAs & Tracks:

curl -s "http://127.0.0.1:8000/api/tewa/da/" | jq -c '.[] | {id, name}'
curl -s "http://127.0.0.1:8000/api/tewa/track/" | jq -c '.[] | {id, track_id}'

Get score breakdown (PK or public id for track_id):

curl -s "http://127.0.0.1:8000/api/tewa/score-breakdown?scenario_id=1&track_id=T1&da_id=3" | jq .
curl -s "http://127.0.0.1:8000/api/tewa/score-breakdown?scenario_id=1&track_id=11&da_id=3" | jq .

Use a specific snapshot time:

curl -s "http://127.0.0.1:8000/api/tewa/score-breakdown?scenario_id=1&track_id=T1&da_id=3&at=2025-10-14T07:38:00Z" | jq .

Implementation Notes / Compatibility

No changes to existing compute engine; we wrap it. If compute returns multiple rows or fails, we fall back to stored ThreatScore.

Legacy consumers relying on flat fields and total_score will keep working.

tdb_s in fallback is a proxy from tdb_km using nominal closure 250 m/s. Replace with actual time once available in the compute output.

Minimal HTML UI (hooks only)

Add a “View breakdown” action from Threat Board rows; point to /api/tewa/score-breakdown?....

A minimal partial target is templates/components/score_breakdown.html (not included here). It should render:

raw metrics, normalized, weights, contributions, final score, computed_at.

Link injection planned in templates/boards/da_board.html.

Caching (Optional P1)

Suggested key: scenario:{id}:track:{id or public}:da:{id}:at:{ts-or-latest}

Integrate in get_score_breakdown compute path before calling engine; invalidate on new compute batch if you add batch IDs.

Where to Continue

Frontend modal partial: templates/components/score_breakdown.html

Threat Board action wiring: templates/boards/da_board.html

Expand tests:

tewa/tests/test_score_breakdown_api.py (already extended)

New template test: tewa/tests/test_templates_breakdown.py (render/200/fields)

If the next agent needs any specific file contents (e.g., ScoreBreakdownSerializer or the service function), ask and I’ll paste exact versions.
